<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM Logger</title>
  <style>
    body {
      font-family: system-ui, sans-serif;
      background: #ffffff;
      color: #222;
      margin: 0;
      padding: 0;
    }
    .container {
      max-width: 800px;
      margin: 0 auto;
      padding: 2rem;
      line-height: 1.6;
    }
    h1 {
      font-size: 2.4rem;
      text-align: center;
    }
    .subtitle {
      text-align: center;
      font-size: 1.2rem;
      margin-bottom: 1.5rem;
    }
    .btn {
      display: inline-block;
      background: #000;
      color: white;
      padding: 0.6rem 1.2rem;
      border-radius: 6px;
      text-decoration: none;
      margin-top: 1rem;
    }
    .center {
      text-align: center;
    }
    img {
      max-width: 100%;
      border: 1px solid #ccc;
      border-radius: 8px;
      margin: 2rem 0;
    }
    ul {
      padding-left: 1.5rem;
    }
    .features {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1rem;
      margin-top: 1rem;
    }
    @media (max-width: 640px) {
      .features {
        grid-template-columns: 1fr;
      }
    }
    footer {
      text-align: center;
      margin-top: 3rem;
      font-size: 0.9rem;
      color: #888;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>üîç LLM Logger</h1>
    <p class="subtitle">A Local UI for Debugging LLMs Step-by-Step</p>
    <div class="center">
      <a class="btn" href="https://github.com/avtark/llm_logger">View on GitHub</a>
      <p><code>pip install llm-logger</code></p>
    </div>

    <img src="demo.gif" alt="Demo of LLM Logger" />

    <h2>What is it?</h2>
    <p>
      <strong>LLM Logger</strong> is a local-first tool for inspecting OpenAI (or Anthropic) API calls.
      It helps you debug chains and agents by logging everything locally and rendering it in a clean UI.
    </p>

    <h2>Features</h2>
    <div class="features">
      <ul>
        <li>üß† Prompt & response diffs between turns</li>
        <li>üìÇ Logs stored locally as JSON</li>
        <li>‚ö° One-line setup via client wrapper</li>
        <li>üîê Fully offline ‚Äî no server, no account</li>
      </ul>
      <ul>
        <li>üîç Tool call & system prompt inspection</li>
        <li>‚è±Ô∏è Metadata + latency tracking</li>
        <li>üñ•Ô∏è Simple, static UI served locally</li>
        <li>üß© Works with any Python codebase</li>
      </ul>
    </div>

    <h2>Why use this?</h2>
    <p>
      Most LLM dev tools are hosted or heavyweight. <strong>llm-logger</strong> gives you a fast, lightweight
      way to understand what your model is doing ‚Äî no cloud needed.
    </p>

    <h2>Get Started</h2>
    <p>Install the package:</p>
    <pre><code>pip install llm-logger</code></pre>

    <p>Then launch the viewer:</p>
    <pre><code>llm_logger</code></pre>
    <p>Or embed the UI in your app using FastAPI or Flask.</p>

    <div class="center">
      <p>üß™ Want to shape v2? <a href="https://github.com/avtark/llm_logger/discussions">Join the discussion</a></p>
    </div>

    <footer>
      Built by <a href="https://twitter.com/avtarkhalsa">@avtarkhalsa</a> ‚Ä¢ MIT Licensed
    </footer>
  </div>
</body>
</html>
